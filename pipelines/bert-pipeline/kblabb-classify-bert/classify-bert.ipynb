{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import exists,basename\n",
    "from json import load,dumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_transformers import BertTokenizer\n",
    "from pytorch_transformers.modeling_bert import BertConfig, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ImportError",
     "evalue": "cannot import name 'Tokenizer' from 'fastai.text' (/home/roger/.local/share/virtualenvs/welfare_state_analytics-0GZ2Cxn6/lib/python3.8/site-packages/fastai/text/__init__.py)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-67f211dad8ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfastai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Tokenizer' from 'fastai.text' (/home/roger/.local/share/virtualenvs/welfare_state_analytics-0GZ2Cxn6/lib/python3.8/site-packages/fastai/text/__init__.py)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ImportError",
     "evalue": "cannot import name 'TextDataBunch' from 'fastai.text.data' (/home/roger/.local/share/virtualenvs/welfare_state_analytics-0GZ2Cxn6/lib/python3.8/site-packages/fastai/text/data.py)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-bfa168501c96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfastai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextDataBunch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfastai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfastai1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfastai1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'TextDataBunch' from 'fastai.text.data' (/home/roger/.local/share/virtualenvs/welfare_state_analytics-0GZ2Cxn6/lib/python3.8/site-packages/fastai/text/data.py)"
     ]
    }
   ],
   "source": [
    "\n",
    "from fastai.text.transform import Tokenizer, BaseTokenizer, List, Vocab\n",
    "from fastai.text.data import TextDataBunch\n",
    "from fastai1.text import *\n",
    "from fastai1.metrics import fbeta, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.classification import confusion_matrix\n",
    "from sklearn.metrics import f1_score,fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fastai.text.transform'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e3f4fa22bccf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrandom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFastAiBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_categories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_batch_bert\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/source/welfare_state_analytics/pipelines/bert-pipeline/kblabb- classify-bert/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_transformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfastai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBaseTokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mVocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfastai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLearner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fastai.text.transform'"
     ]
    }
   ],
   "source": [
    "from confusion import get_confusion_plot, plot_confusion_matrix\n",
    "from sklearn.metrics.classification import confusion_matrix\n",
    "from random import random\n",
    "from utils import Config, FastAiBertTokenizer, fill_categories, fill, accuracy, loss_batch_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = '../data_swepub/E2.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PER_CATEGORY=None\n",
    "N_MAX=None\n",
    "seed = 42 # use constant seed for predictability\n",
    "seed = seed or int(10000 * random()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data = read_csv(data_file)\n",
    "\n",
    "# randomize\n",
    "data = data.sample(frac=1, random_state=seed)\n",
    "\n",
    "# create 'text' column and put it first\n",
    "data['text'] = data['title'] + \" \" + data['abstract'] \n",
    "data = data[data.columns.tolist()[:1] + data.columns.tolist()[-1:] + data.columns.tolist()[1:-1]]\n",
    "data = data.fillna('')\n",
    "\n",
    "text_col = 'text'\n",
    "first_label_index=8 # <- be sure to use correct index\n",
    "label_cols = list(data.columns[first_label_index:]) \n",
    "\n",
    "# filter data\n",
    "\n",
    "# calculate sums\n",
    "sums = DataFrame({ label: [ sum(data[label]) ] for label in label_cols })\n",
    "\n",
    "train, val = train_test_split(data, train_size=0.6)\n",
    "val, test = train_test_split(val, train_size=0.5)\n",
    "\n",
    "# equalize categories?\n",
    "if N_PER_CATEGORY:\n",
    "    train = fill_categories(train, label_cols, n=N_PER_CATEGORY, random_state=seed+1)\n",
    "\n",
    "if N_MAX:\n",
    "    train = train.sample(n=N_MAX, random_state=seed+2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orgs = set()\n",
    "for org in data['organisation']:\n",
    "    for o in org.split(';'):\n",
    "        if o.endswith('.se'):\n",
    "            orgs.add(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data), len(train), len(val), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(\n",
    "    testing=False,\n",
    "    model_name=\"bert-large-cased\",\n",
    "    max_lr=3e-5,\n",
    "    epochs=2,\n",
    "    use_fp16=True,\n",
    "    bs=64,\n",
    "    discriminative=False,\n",
    "    max_seq_len=256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tok = BertTokenizer.from_pretrained(config.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastai_bert_vocab = Vocab(list(bert_tok.vocab.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastai_tokenizer = Tokenizer(\n",
    "                        tok_func=FastAiBertTokenizer(\n",
    "                                    bert_tok, \n",
    "                                    max_seq_len=config.max_seq_len),\n",
    "                        pre_rules=[],\n",
    "                        post_rules=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data into a databunch \n",
    "databunch = TextDataBunch.from_df(\".\", train, val, test,\n",
    "    tokenizer=fastai_tokenizer, vocab=fastai_bert_vocab,\n",
    "    include_bos=False,\n",
    "    include_eos=False, \n",
    "    text_cols=text_col, \n",
    "    label_cols=label_cols, \n",
    "    bs=config.bs,\n",
    "    collate_fn=partial(pad_collate, pad_first=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pytorch_pretrained_bert.modeling import BertConfig, BertForSequenceClassification\n",
    "bert_model = BertForSequenceClassification.from_pretrained(config.model_name, num_labels=len(label_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To change the loss_batch function in the loaded fastai module\n",
    "module_basic_train = sys.modules['fastai.basic_train']\n",
    "module_basic_train.loss_batch = loss_batch_bert\n",
    "sys.modules['fastai.basic_train'] = module_basic_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = partial(fbeta, beta=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = Learner(\n",
    "            databunch, bert_model,\n",
    "            loss_func=loss_func,\n",
    "            metrics=[ fbeta, f1, accuracy ]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.use_fp16:\n",
    "    learner = learner.to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.callbacks.append(ShowGraph(learner))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learner.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit_one_cycle(config.epochs, max_lr=config.max_lr, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, y_true = learner.get_preds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = basename(data_file)\n",
    "dataset = dataset[0:dataset.index('.')]\n",
    "accuracy = (y_true.argmax(axis=1) == preds.argmax(axis=1)).float().mean()\n",
    "f1 = f1_score(y_true.argmax(axis=1), preds.argmax(axis=1), average='weighted')\n",
    "f2 = fbeta_score(y_true.argmax(axis=1), preds.argmax(axis=1), beta=2, average='weighted')\n",
    "cm = confusion_matrix(y_true.argmax(axis=1), preds.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title=f'Dataset={dataset}, model={config.model_name}, max_lr={config.max_lr}, {(\"seed=\" + str(seed) + \", \") if seed else \"\"}{(\"N per category=\" + str(N_PER_CATEGORY) + \", \") if N_PER_CATEGORY else \"\"}ntrain={len(train)}, nval={len(val)}, ntest={len(test)}, Acc={accuracy:.3f}, F1={f1:.3f}, F2={f2:.3f}'\n",
    "c = get_confusion_plot(cm=cm, normalize=False, labels=list(label_cols), title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'results/result_{int(10000*random())}.txt', mode='w') as f:\n",
    "    f.write(dumps(\n",
    "        {\n",
    "          'seed': seed,\n",
    "          'dataset': basename(data_file),\n",
    "          'train_size': len(train),\n",
    "          'val_size': len(val),\n",
    "          'test_size': len(val),\n",
    "          'model': config.model_name,\n",
    "          'max_lr': config.max_lr,\n",
    "          'batch_size': config.bs,\n",
    "          'max_seq_length': config.max_seq_len,\n",
    "          'n_per_category': N_PER_CATEGORY,\n",
    "          'n_max': N_MAX,\n",
    "          'accuracy': float(accuracy),\n",
    "          'f1': float(f1),\n",
    "          'f2': float(f2),\n",
    "          'labels': label_cols,\n",
    "          'confusion': cm.tolist()\n",
    "        }\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame(preds[0:10].tolist(), columns=label_cols).style.background_gradient(axis=1, cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame(y_true[0:10].tolist(), columns=label_cols).style.background_gradient(axis=1, cmap='Blues')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "kernelspec": {
   "name": "python_defaultSpec_1599330012957",
   "display_name": "Python 3.8.5 64-bit ('welfare_state_analytics': pipenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}